{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f41e20f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime\n",
    "from azure.storage.blob import BlobServiceClient, generate_blob_sas, BlobSasPermissions\n",
    "from openai import AzureOpenAI\n",
    "from dotenv import load_dotenv\n",
    "import json\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import Literal\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import threading\n",
    "from functools import partial\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6721d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables\n",
    "load_dotenv(override=True)\n",
    "\n",
    "# Azure Storage Blob configuration\n",
    "BLOB_CONNECTION_STRING = os.getenv(\"BLOB_CONNECTION_STRING\")\n",
    "BLOB_CONTAINER_NAME = os.getenv(\"BLOB_CONTAINER_NAME\")\n",
    "\n",
    "# Azure OpenAI configuration\n",
    "AZURE_OPENAI_ENDPOINT = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "AZURE_OPENAI_API_KEY = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "AOAI_API_VERSION = '2025-03-01-preview'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4142b645",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_blob_service_client():\n",
    "    \"\"\"Returns a BlobServiceClient instance.\"\"\"\n",
    "    return BlobServiceClient.from_connection_string(BLOB_CONNECTION_STRING)\n",
    "\n",
    "def upload_to_blob(file_path, blob_name):\n",
    "    \"\"\"\n",
    "    Uploads a file to Azure Blob Storage and returns its URL with SAS token.\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path to the local file\n",
    "        blob_name: Name to use in blob storage\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (blob_url, sas_token)\n",
    "    \"\"\"\n",
    "    blob_service_client = get_blob_service_client()\n",
    "    blob_client = blob_service_client.get_blob_client(container=BLOB_CONTAINER_NAME, blob=blob_name)\n",
    "    \n",
    "    with open(file_path, \"rb\") as data:\n",
    "        blob_client.upload_blob(data, overwrite=True)\n",
    "    \n",
    "    start_time = datetime.datetime.now(datetime.timezone.utc)\n",
    "    expiry_time = start_time + datetime.timedelta(days=1)\n",
    "\n",
    "    # Generate SAS token\n",
    "    sas_token = generate_blob_sas(\n",
    "        account_name=blob_client.account_name,\n",
    "        container_name=blob_client.container_name,\n",
    "        blob_name=blob_client.blob_name,\n",
    "        account_key=blob_service_client.credential.account_key,\n",
    "        permission=BlobSasPermissions(read=True),\n",
    "        expiry=expiry_time,\n",
    "        start=start_time\n",
    "    )\n",
    "    \n",
    "    blob_url = blob_client.url\n",
    "    return blob_url, sas_token\n",
    "\n",
    "def get_openai_client():\n",
    "    \"\"\"Returns an AzureOpenAI client instance for GPT-4o model.\"\"\"\n",
    "    client = AzureOpenAI(\n",
    "        azure_endpoint=AZURE_OPENAI_ENDPOINT,\n",
    "        api_key=AZURE_OPENAI_API_KEY,\n",
    "        api_version=AOAI_API_VERSION\n",
    "    )\n",
    "    return client\n",
    "\n",
    "class ClassificationResult(BaseModel):\n",
    "    \"\"\"部品外観検査の分類結果を表すPydanticモデル。\"\"\"\n",
    "    classification: Literal[\"OK\", \"汚れ\", \"欠け\", \"削り節\"]\n",
    "    reasoning: str\n",
    "    confidence: int = Field(\n",
    "        ...,\n",
    "        ge=1,\n",
    "        le=10,\n",
    "        description=\"分類の確信度（1〜10の整数）\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa328a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_images_from_folder(folder_path):\n",
    "    \"\"\"\n",
    "    Upload all images from a folder structure to Blob Storage.\n",
    "    \n",
    "    Args:\n",
    "        folder_path: Path to the input folder\n",
    "        \n",
    "    Returns:\n",
    "        dict: Dictionary with image paths as keys and (blob_url, sas_token) as values\n",
    "    \"\"\"\n",
    "    uploaded_images = {}\n",
    "    folder_path = Path(folder_path)\n",
    "    \n",
    "    for image_file in folder_path.rglob(\"*.png\"):\n",
    "        # Create blob name preserving folder structure\n",
    "        relative_path = image_file.relative_to(folder_path)\n",
    "        blob_name = str(relative_path).replace(\"\\\\\", \"/\")\n",
    "        \n",
    "        try:\n",
    "            blob_url, sas_token = upload_to_blob(str(image_file), blob_name)\n",
    "            full_url = f\"{blob_url}?{sas_token}\"\n",
    "            uploaded_images[str(image_file)] = full_url\n",
    "            print(f\"Uploaded: {blob_name}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error uploading {image_file}: {e}\")\n",
    "    \n",
    "    return uploaded_images\n",
    "\n",
    "def load_sample_images(uploaded_images, sample_folder=\"sample\"):\n",
    "    \"\"\"\n",
    "    Load sample images for few-shot learning.\n",
    "    \n",
    "    Args:\n",
    "        uploaded_images: Dictionary of uploaded images\n",
    "        sample_folder: Folder name to look for (default: \"sample\")\n",
    "    \n",
    "    Returns:\n",
    "        dict: Dictionary with labels as keys and list of image URLs as values\n",
    "    \"\"\"\n",
    "    samples = {\"OK\": [], \"汚れ\": [], \"欠け\": [], \"削り節\": []}\n",
    "    \n",
    "    for image_path, url in uploaded_images.items():\n",
    "        # Normalize path separators and check if it contains sample folder\n",
    "        normalized_path = image_path.replace(\"\\\\\", \"/\")\n",
    "        \n",
    "        if f\"/{sample_folder}/\" in normalized_path or normalized_path.endswith(f\"/{sample_folder}\"):\n",
    "            # Extract label from path - look for any of our target labels\n",
    "            for label in samples.keys():\n",
    "                if f\"/{label}/\" in normalized_path or f\"\\\\{label}\\\\\" in image_path:\n",
    "                    samples[label].append(url)\n",
    "                    print(f\"Added {label} sample: {Path(image_path).name}\")\n",
    "                    break\n",
    "    \n",
    "    return samples\n",
    "\n",
    "def create_few_shot_messages(sample_images):\n",
    "    \"\"\"\n",
    "    Create few-shot messages for OpenAI API.\n",
    "    \"\"\"\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"developer\",\n",
    "            \"content\": \"\"\"あなたは製造業の品質検査の専門家です。部品の画像を見て、以下の4つのカテゴリーに分類してください。必ず「分類結果」「理由」「確信度（1–5の5段階）」の３つを出力してください。\n",
    "\n",
    "\t\t\t\t─── 分類定義 ───\n",
    "\t\t\t\t• OK  \n",
    "\t\t\t\t– 定義：製品の外観に異常がなく、形状・表面状態ともに規定の仕様範囲内。  \n",
    "\t\t\t\t– 判定根拠例：  \n",
    "\t\t\t\t\t• 表面に凹凸や飛び出しがない  \n",
    "\t\t\t\t\t• 欠け・割れ・ヒビが見当たらない  \n",
    "\t\t\t\t\t• 汚れや異物付着が確認できない  \n",
    "\n",
    "\t\t\t\t• 削り節  \n",
    "\t\t\t\t– 定義：製造工程で発生した切削くずが製品上部やエッジに残留・飛び出しているもの。  \n",
    "\t\t\t\t– 判定根拠例：  \n",
    "\t\t\t\t\t• 細長い切削片（くず状／糸くず状）が付着  \n",
    "\t\t\t\t\t• 本体素材と質感・色がわずかに異なり「くず」と認識できる  \n",
    "\t\t\t\t\t• 複数箇所に飛び出し・突起がある  \n",
    "\n",
    "\t\t\t\t• 欠け  \n",
    "\t\t\t\t– 定義：表面またはエッジの一部が物理的に欠落し、凹状の傷・穴があるもの。  \n",
    "\t\t\t\t– 判定根拠例：  \n",
    "\t\t\t\t\t• エッジが不自然に丸くなっている、あるいはギザギザ  \n",
    "\t\t\t\t\t• 表面材が剥離し底地（金属・樹脂芯）が見える  \n",
    "\t\t\t\t\t• 欠落部分の輪郭がシャープで「欠け」と特定できる  \n",
    "\n",
    "\t\t\t\t• 汚れ  \n",
    "\t\t\t\t– 定義：表面に油脂・ほこり・粉じん・液体シミなどの異物が付着し、拭いても残るもの。  \n",
    "\t\t\t\t– 判定根拠例：  \n",
    "\t\t\t\t\t• 基材と異なる色調の斑点・筋状シミ・塊  \n",
    "\t\t\t\t\t• 汚れ部分だけ光沢が落ちマットに見える  \n",
    "\t\t\t\t\t• 触れるとべたつきやざらつきを感じる  \n",
    "\n",
    "\t\t\t\t─── 分析手順 ───\n",
    "\t\t\t\t1. 画像全体を俯瞰し、形状・輪郭の歪みや突出をチェック  \n",
    "\t\t\t\t2. 表面テクスチャと色調を観察し、「OK」と「汚れ」の区別点を確認  \n",
    "\t\t\t\t3. 欠けや切削くずの特徴（シャープさ、糸くず状など）を拡大して詳細に観察  \n",
    "\t\t\t\t4. 各定義と照合し、最も当てはまるラベルを選択  \n",
    "\n",
    "\t\t\t\t─── 確信度定義（1–5）───\n",
    "\t\t\t\t1. 根拠ほぼなし：特徴が不明瞭で他ラベルの可能性も高い  \n",
    "\t\t\t\t2. 根拠弱い：一部該当する箇所はあるが曖昧・画像が不鮮明  \n",
    "\t\t\t\t3. 標準的：定義に合う特徴が確認でき、他の候補は比較的低い  \n",
    "\t\t\t\t4. 高い：複数の明確な根拠が揃っており、ほぼ間違いない  \n",
    "\t\t\t\t5. 非常に高い：決定的な特徴が圧倒的に一致、誤分類の余地なし  \n",
    "\n",
    "\t\t\t\t─── 出力フォーマット ───\n",
    "\t\t\t\t分類結果: <OK／汚れ／欠け／削り節>  \n",
    "\t\t\t\t理由: <具体的な観察ポイントと定義との照合結果を詳細に記述>  \n",
    "\t\t\t\t確信度: <1〜5の整数で、上記定義に沿って評価>\n",
    "\t\t\t\t\"\"\"\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    # Define detailed reasoning templates for each category\n",
    "    reasoning_templates = {\n",
    "        \"OK\": [\n",
    "            \"画像を詳細に観察した結果、製品の表面は滑らかで均一な仕上がりを示しており、欠け・割れ・ヒビは一切確認できません。エッジ部分も設計通りの形状を保持し、異物付着や汚れも見当たりません。全体的に規定の仕様範囲内の良好な状態です。\",\n",
    "            \"製品全体を俯瞰したところ、形状・輪郭に歪みや突出はなく、表面テクスチャも一様で異常は認められません。切削くずの残留や表面剥離の兆候もなく、品質基準を満たしている状態と判断します。\"\n",
    "        ],\n",
    "        \"汚れ\": [\n",
    "            \"表面に基材とは異なる色調の斑点状の付着物が複数箇所確認できます。これらの汚れ部分は光沢が落ちてマット状に見え、油脂系または粉じんの付着と考えられます。拭き取りでは除去困難な定着した汚れの特徴を示しています。\",\n",
    "            \"画像中央付近に筋状のシミと、周辺部に点状の汚れが観察されます。汚れ部分の色調が基材と明らかに異なり、表面の質感も変化していることから、外部からの異物付着による汚れと断定します。\"\n",
    "        ],\n",
    "        \"欠け\": [\n",
    "            \"エッジ部分に明確な欠落が確認でき、本来の直線的な輪郭が不自然に丸くなっています。欠落部分の境界はシャープで、物理的な衝撃による材料の剥離と判断されます。底地の素材も一部露出しており、典型的な「欠け」の特徴を示しています。\",\n",
    "            \"製品表面に凹状の傷が複数箇所見られ、特に角部において材料の欠落が顕著です。欠け部分の輪郭が明瞭で、表面コーティングが剥がれて下地が見える状況から、物理的損傷による欠けと特定できます。\"\n",
    "        ],\n",
    "        \"削り節\": [\n",
    "            \"製品上部とエッジ部分に細長い糸くず状の切削片が複数付着しているのが確認できます。これらの切削くずは本体素材とは質感が異なり、製造工程で発生した切削屑が除去されずに残留したものと判断されます。\",\n",
    "            \"表面に複数の突起状の飛び出しが観察され、これらは製造時の切削加工で発生したくず片の付着と考えられます。くず片の形状が細長く、本体部分とは色調や質感が微妙に異なることから削り節の特徴と一致します。\"\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    # Add few-shot examples\n",
    "    for label, urls in sample_images.items():\n",
    "        reasoning_list = reasoning_templates.get(label, [f\"この画像は{label}の特徴を示しています。\"])\n",
    "        \n",
    "        for i, url in enumerate(urls[:4]):  # Use first 2 samples per category\n",
    "            reasoning = reasoning_list[i % len(reasoning_list)]\n",
    "            \n",
    "            # Set appropriate confidence based on category\n",
    "            confidence = 4 if label in [\"OK\", \"欠け\"] else 3\n",
    "            \n",
    "            messages.extend([\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": [\n",
    "                        {\n",
    "                            \"type\": \"text\",\n",
    "                            \"text\": \"この部品の画像を分類してください。\"\n",
    "                        },\n",
    "                        {\n",
    "                            \"type\": \"image_url\",\n",
    "                            \"image_url\": {\"url\": url, \"detail\": \"high\"}\n",
    "                        }\n",
    "                    ]\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"assistant\",\n",
    "                    \"content\": json.dumps({\n",
    "                        \"classification\": label,\n",
    "                        \"reasoning\": reasoning,\n",
    "                        \"confidence\": confidence\n",
    "                    }, ensure_ascii=False)\n",
    "                }\n",
    "            ])\n",
    "    \n",
    "    return messages\n",
    "\n",
    "def classify_image(client, image_url, few_shot_messages):\n",
    "    \"\"\"\n",
    "    Classify a single image using Azure OpenAI with Pydantic structured output.\n",
    "    \"\"\"\n",
    "    messages = few_shot_messages.copy()\n",
    "    messages.append({\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\n",
    "                \"type\": \"text\",\n",
    "                \"text\": \"この部品の画像を分類してください。\"\n",
    "            },\n",
    "            {\n",
    "                \"type\": \"image_url\",\n",
    "                \"image_url\": {\"url\": image_url, \"detail\": \"high\"}\n",
    "            }\n",
    "        ]\n",
    "    })\n",
    "    \n",
    "    try:\n",
    "        completion = client.beta.chat.completions.parse(\n",
    "            model=\"o4-mini\",  # Replace with your actual model deployment name\n",
    "            messages=messages,\n",
    "            response_format=ClassificationResult,\n",
    "            max_completion_tokens=1000\n",
    "        )\n",
    "        \n",
    "        result = completion.choices[0].message.parsed\n",
    "        return {\n",
    "            \"classification\": result.classification,\n",
    "            \"reasoning\": result.reasoning,\n",
    "            \"confidence\": result.confidence\n",
    "        }\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error classifying image: {e}\")\n",
    "        return {\"classification\": \"ERROR\", \"reasoning\": f\"Error: {e}\"}\n",
    "\n",
    "def classify_image_wrapper(args):\n",
    "    \"\"\"\n",
    "    Wrapper function for parallel classification with better error handling.\n",
    "    \n",
    "    Args:\n",
    "        args: Tuple of (image_url, true_label, image_path, client, few_shot_messages, index, total)\n",
    "    \n",
    "    Returns:\n",
    "        dict: Classification result with metadata\n",
    "    \"\"\"\n",
    "    image_url, true_label, image_path, client, few_shot_messages, index, total = args\n",
    "    \n",
    "    print(f\"[{index+1}/{total}] Classifying: {Path(image_path).name}\")\n",
    "    \n",
    "    try:\n",
    "        result = classify_image(client, image_url, few_shot_messages)\n",
    "        \n",
    "        return {\n",
    "            'image_path': image_path,\n",
    "            'image_url': image_url,\n",
    "            'true_label': true_label,\n",
    "            'predicted': result['classification'],\n",
    "            'reasoning': result['reasoning'],\n",
    "            'confidence': result.get('confidence', 0),\n",
    "            'status': 'success',\n",
    "            'index': index\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Error classifying {Path(image_path).name}: {e}\")\n",
    "        return {\n",
    "            'image_path': image_path,\n",
    "            'image_url': image_url,\n",
    "            'true_label': true_label,\n",
    "            'predicted': 'ERROR',\n",
    "            'reasoning': f'Classification failed: {str(e)}',\n",
    "            'confidence': 0,\n",
    "            'status': 'error',\n",
    "            'index': index\n",
    "        }\n",
    "\n",
    "def classify_images_parallel(test_data, client, few_shot_messages, max_workers=5):\n",
    "    \"\"\"\n",
    "    Classify images in parallel using ThreadPoolExecutor.\n",
    "    \n",
    "    Args:\n",
    "        test_data: List of (image_url, true_label, image_path) tuples\n",
    "        client: Azure OpenAI client\n",
    "        few_shot_messages: Few-shot messages for classification\n",
    "        max_workers: Maximum number of parallel workers\n",
    "    \n",
    "    Returns:\n",
    "        list: List of classification results\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    total = len(test_data)\n",
    "    \n",
    "    # Prepare arguments for parallel processing\n",
    "    args_list = [\n",
    "        (image_url, true_label, image_path, client, few_shot_messages, i, total)\n",
    "        for i, (image_url, true_label, image_path) in enumerate(test_data)\n",
    "    ]\n",
    "    \n",
    "    print(f\"Starting parallel classification with {max_workers} workers...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        # Submit all tasks\n",
    "        future_to_args = {executor.submit(classify_image_wrapper, args): args for args in args_list}\n",
    "        \n",
    "        # Collect results as they complete\n",
    "        for future in as_completed(future_to_args):\n",
    "            try:\n",
    "                result = future.result()\n",
    "                results.append(result)\n",
    "                \n",
    "                # Print progress\n",
    "                completed = len(results)\n",
    "                elapsed = time.time() - start_time\n",
    "                avg_time = elapsed / completed\n",
    "                eta = avg_time * (total - completed)\n",
    "                \n",
    "                if completed % 5 == 0 or completed == total:\n",
    "                    print(f\"Progress: {completed}/{total} ({completed/total*100:.1f}%) - \"\n",
    "                          f\"ETA: {eta:.1f}s\")\n",
    "                          \n",
    "            except Exception as e:\n",
    "                print(f\"Future failed: {e}\")\n",
    "    \n",
    "    # Sort results by index to maintain order\n",
    "    results.sort(key=lambda x: x['index'])\n",
    "    \n",
    "    total_time = time.time() - start_time\n",
    "    print(f\"\\nClassification completed in {total_time:.1f}s\")\n",
    "    print(f\"Average time per image: {total_time/total:.2f}s\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "def classify_images_by_folder_parallel(test_data, client, few_shot_messages, max_workers=3):\n",
    "    \"\"\"\n",
    "    Classify images in parallel, processing each folder (label) separately.\n",
    "    \n",
    "    Args:\n",
    "        test_data: List of (image_url, true_label, image_path) tuples\n",
    "        client: Azure OpenAI client\n",
    "        few_shot_messages: Few-shot messages for classification\n",
    "        max_workers: Maximum number of parallel workers per folder\n",
    "    \n",
    "    Returns:\n",
    "        list: List of classification results\n",
    "    \"\"\"\n",
    "    # Group test data by true label\n",
    "    grouped_data = {}\n",
    "    for item in test_data:\n",
    "        true_label = item[1]\n",
    "        if true_label not in grouped_data:\n",
    "            grouped_data[true_label] = []\n",
    "        grouped_data[true_label].append(item)\n",
    "    \n",
    "    all_results = []\n",
    "    \n",
    "    print(\"Processing folders in parallel...\")\n",
    "    \n",
    "    # Process each folder in parallel\n",
    "    folder_futures = {}\n",
    "    with ThreadPoolExecutor(max_workers=len(grouped_data)) as folder_executor:\n",
    "        for label, folder_data in grouped_data.items():\n",
    "            print(f\"Submitting {len(folder_data)} images from '{label}' folder\")\n",
    "            future = folder_executor.submit(\n",
    "                classify_images_parallel, \n",
    "                folder_data, \n",
    "                client, \n",
    "                few_shot_messages, \n",
    "                max_workers\n",
    "            )\n",
    "            folder_futures[future] = label\n",
    "        \n",
    "        # Collect results from each folder\n",
    "        for future in as_completed(folder_futures):\n",
    "            label = folder_futures[future]\n",
    "            try:\n",
    "                folder_results = future.result()\n",
    "                all_results.extend(folder_results)\n",
    "                print(f\"Completed processing '{label}' folder: {len(folder_results)} images\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing '{label}' folder: {e}\")\n",
    "    \n",
    "    # Sort all results by original index\n",
    "    all_results.sort(key=lambda x: x['index'])\n",
    "    \n",
    "    return all_results\n",
    "\n",
    "def load_test_images(uploaded_images, test_folder=\"test\"):\n",
    "    \"\"\"\n",
    "    Load test images with their true labels.\n",
    "    \n",
    "    Args:\n",
    "        uploaded_images: Dictionary of uploaded images\n",
    "        test_folder: Folder name to look for (default: \"test\")\n",
    "    \n",
    "    Returns:\n",
    "        list: List of tuples (image_url, true_label, image_path)\n",
    "    \"\"\"\n",
    "    test_data = []\n",
    "    \n",
    "    for image_path, url in uploaded_images.items():\n",
    "        # Normalize path separators and check if it contains test folder\n",
    "        normalized_path = image_path.replace(\"\\\\\", \"/\")\n",
    "        \n",
    "        if f\"/{test_folder}/\" in normalized_path:\n",
    "            # Extract true label from folder structure\n",
    "            for label in [\"OK\", \"汚れ\", \"欠け\", \"削り節\"]:\n",
    "                if f\"/{label}/\" in normalized_path or f\"\\\\{label}\\\\\" in image_path:\n",
    "                    test_data.append((url, label, image_path))\n",
    "                    break\n",
    "    \n",
    "    return test_data\n",
    "\n",
    "def evaluate_results(results_df):\n",
    "    \"\"\"\n",
    "    Evaluate classification results and create confusion matrix.\n",
    "    \"\"\"\n",
    "    # Calculate accuracy\n",
    "    accuracy = (results_df['predicted'] == results_df['true_label']).mean()\n",
    "    print(f\"Accuracy: {accuracy:.3f}\")\n",
    "    \n",
    "    # Classification report\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(results_df['true_label'], results_df['predicted']))\n",
    "    \n",
    "    # Confusion matrix\n",
    "    labels = [\"OK\", \"汚れ\", \"欠け\", \"削り節\"]\n",
    "    cm = confusion_matrix(results_df['true_label'], results_df['predicted'], labels=labels)\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=labels, yticklabels=labels)\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.show()\n",
    "    \n",
    "    return accuracy, cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df8989ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload all images from input/背景カットなし folder only\n",
    "print(\"Uploading images from 背景カットなし folder to Blob Storage...\")\n",
    "uploaded_images = upload_images_from_folder(\"input/背景カットなし\")\n",
    "print(f\"Uploaded {len(uploaded_images)} images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a732d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load sample images for few-shot learning\n",
    "sample_images = load_sample_images(uploaded_images)\n",
    "print(\"Sample images loaded:\")\n",
    "for label, urls in sample_images.items():\n",
    "    print(f\"{label}: {len(urls)} images\")\n",
    "\n",
    "# Create few-shot messages\n",
    "few_shot_messages = create_few_shot_messages(sample_images)\n",
    "print(f\"Created {len(few_shot_messages)} few-shot messages\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dc656d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "few_shot_messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38cebaf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test images\n",
    "test_data = load_test_images(uploaded_images)\n",
    "print(f\"Loaded {len(test_data)} test images\")\n",
    "\n",
    "# Initialize OpenAI client\n",
    "client = get_openai_client()\n",
    "\n",
    "# Choose parallel processing method\n",
    "use_folder_parallel = True  # Set to False for simple parallel processing\n",
    "max_workers = 4  # Adjust based on your Azure OpenAI rate limits\n",
    "\n",
    "if use_folder_parallel:\n",
    "    print(\"Using folder-based parallel processing...\")\n",
    "    results = classify_images_by_folder_parallel(test_data, client, few_shot_messages, max_workers)\n",
    "else:\n",
    "    print(\"Using simple parallel processing...\")\n",
    "    results = classify_images_parallel(test_data, client, few_shot_messages, max_workers)\n",
    "\n",
    "# Convert to DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Remove the index column and status column if not needed for evaluation\n",
    "results_df = results_df.drop(['index', 'status'], axis=1, errors='ignore')\n",
    "\n",
    "print(\"\\nClassification completed!\")\n",
    "print(results_df[['image_path', 'true_label', 'predicted']].head())\n",
    "\n",
    "# Show error summary\n",
    "error_count = len(results_df[results_df['predicted'] == 'ERROR'])\n",
    "if error_count > 0:\n",
    "    print(f\"\\nWarning: {error_count} images failed to classify\")\n",
    "    print(\"Error details:\")\n",
    "    error_df = results_df[results_df['predicted'] == 'ERROR']\n",
    "    for _, row in error_df.iterrows():\n",
    "        print(f\"  {Path(row['image_path']).name}: {row['reasoning']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f08721e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate results (excluding error cases)\n",
    "valid_results_df = results_df[results_df['predicted'] != 'ERROR'].copy()\n",
    "\n",
    "print(f\"Evaluation Results (excluding {len(results_df) - len(valid_results_df)} error cases):\")\n",
    "accuracy, confusion_mat = evaluate_results(valid_results_df)\n",
    "\n",
    "# Save detailed results\n",
    "output_dir = 'analysis-results'\n",
    "if not os.path.exists(output_dir):\n",
    "\tos.makedirs(output_dir)\n",
    "results_df.to_csv(f'{output_dir}/classification_results.csv', index=False, encoding='utf-8-sig')\n",
    "print(\"\\nDetailed results saved to 'classification_results.csv'\")\n",
    "\n",
    "# Display some example predictions\n",
    "print(\"\\nSample predictions:\")\n",
    "for idx, row in valid_results_df.head().iterrows():\n",
    "    print(f\"\\nImage: {Path(row['image_path']).name}\")\n",
    "    print(f\"True: {row['true_label']}, Predicted: {row['predicted']}\")\n",
    "    print(f\"Confidence: {row['confidence']}\")\n",
    "    print(f\"Reasoning: {row['reasoning'][:100]}...\")\n",
    "\n",
    "# Display confidence distribution\n",
    "print(f\"\\nConfidence distribution:\")\n",
    "confidence_stats = valid_results_df['confidence'].describe()\n",
    "print(confidence_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a6543b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
